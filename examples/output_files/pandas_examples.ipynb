{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Pandas cheat code"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Introduction\n",
            "The objective of this knowledge post is to save time and increase code quality by offering an alternative to Pandas documentation and Stake Overflow in the form of a Jupyter Notebook with the following features:\n",
            "\n",
            "- Pandas key data transformation easy to copy-past code snippets\n",
            "- Embedded data for easy snippets run and testing\n",
            "- Alteryx nodes to Pandas functions mapping\n",
            "\n",
            "## How to leverage it\n",
            "- you can run and test any function with embedded data, just make sure to load the data first ;)\n",
            "- you can easily copy-past snippet of codes by **double clicking** on a cell, right click and copy-paste"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Contents"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {
            "collapsed": true
         },
         "source": [
            "### 1. Getting started\n",
            "\n",
            "The first time you use this notebook, run once the pip install libraries (in case not already installed on your Python environment)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 1.1. Setup libraries"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 24,
         "metadata": {},
         "outputs": [],
         "source": [
            "!pip install pandas\n",
            "!pip install openpyxl"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 25,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 1.2. About pandas dataframes"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "A pandas dataframe contains an array, a list of column names (the column index) and a list of row names (the row index) In many cases, an entry in the column index is the name of the column and the row index is the row number.  Some operations on dataframes affect the row index, so the name of the row is no longer a row number. We will see how to fix that in xx.xx.\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 2. Read data (inputdata)\n",
            "https://pandas.pydata.org/docs/user_guide/io.html"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<div class=\"alert alert-block alert-danger\">\n",
            "Make sure to load the datasets before running any other snippets of code ;)\n",
            "    </div>"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 2.1. Load text file (.csv, .txt, ...)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Transfer the content of a file to a dataframe and get basic information about the loaded dataset. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 56,
         "metadata": {},
         "outputs": [],
         "source": [
            "#\n",
            "# read a comma separated file\n",
            "#\n",
            "transactions = pd.read_csv('data_input/transactions.csv', sep=',')\n",
            "\n",
            "#\n",
            "# read a tab separated file\n",
            "#\n",
            "customers_1 = pd.read_csv('data_input/customers.txt', sep='\\t')\n",
            "\n",
            "# note that the phone numbers are loosing their leading zeroes:\n",
            "print(customers_1)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 57,
         "metadata": {},
         "outputs": [],
         "source": [
            "#\n",
            "# we need to prevent pandas to convert loaded text to a numeric field type\n",
            "#\n",
            "# read a text file (here a tab separated file) and force all fields to string\n",
            "#\n",
            "customers = pd.read_csv('data_input/customers.txt', sep='\\t', dtype='str')\n",
            "print(customers)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 2.2. Load xlsx"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Pandas uses openpyxl to load a dataset from a sheet. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 28,
         "metadata": {},
         "outputs": [],
         "source": [
            "costs = pd.read_excel('data_input/costs.xlsx', sheet_name='Sheet1')"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 2.3 Get basic information about the loaded dataset"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Get the number of rows, number of columns and list of column names"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 29,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "print(f\"Nomber of rows: {customers.shape[0]}\")\n",
            "print(f\"Nomber of columns: {customers.shape[1]}\")\n",
            "print(\"Columns:\", customers.columns)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 3. Explore data (browse)\n",
            "https://pandas.pydata.org/docs/user_guide/basics.html"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 3.1. Remove column number display limitation"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 30,
         "metadata": {},
         "outputs": [],
         "source": [
            "pd.set_option('display.max_columns', None)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 3.2. Display top n rows"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 31,
         "metadata": {},
         "outputs": [],
         "source": [
            "transactions.head(n=5)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 3.3. Display all rows"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 32,
         "metadata": {},
         "outputs": [],
         "source": [
            "transactions"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 3.4. Quick statistic of numerical columns"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 33,
         "metadata": {},
         "outputs": [],
         "source": [
            "transactions.describe()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 3.5. Distinct values of a column"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 34,
         "metadata": {},
         "outputs": [],
         "source": [
            "#\n",
            "# count number of distinct values in a column\n",
            "#\n",
            "print(transactions[\"customer_id\"].nunique())"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 35,
         "metadata": {},
         "outputs": [],
         "source": [
            "#\n",
            "# list of distinct values in a column\n",
            "#\n",
            "print(transactions[\"customer_id\"].unique())"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 36,
         "metadata": {},
         "outputs": [],
         "source": [
            "#\n",
            "# list of distinct values in a column with their frequency\n",
            "#\n",
            "print(transactions[\"customer_id\"].value_counts())"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 4. Select data (select)\n",
            "https://pandas.pydata.org/docs/user_guide/basics.html"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 4.1. Display columns and types"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 37,
         "metadata": {},
         "outputs": [],
         "source": [
            "transactions.dtypes"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 38,
         "metadata": {},
         "outputs": [],
         "source": [
            "transactions.info()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 4.2. Display number of rows"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 39,
         "metadata": {},
         "outputs": [],
         "source": [
            "transactions.shape[0]"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 4.3. Keep only a subset of columns"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 40,
         "metadata": {},
         "outputs": [],
         "source": [
            "transactions_subset = transactions.copy()\n",
            "transactions_subset = transactions_subset[['customer_id', 'product_id']]\n",
            "transactions_subset.dtypes"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 4.4. Drop columns"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 41,
         "metadata": {},
         "outputs": [],
         "source": [
            "transactions_drop = transactions.copy()\n",
            "transactions_drop.drop(['row_id', 'geo', 'bu', 'customer_level_2', 'product_level_2'], axis=1, inplace=True)\n",
            "transactions_drop.dtypes"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 4.5. Rename columns"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 42,
         "metadata": {},
         "outputs": [],
         "source": [
            "transactions_rename = transactions.copy()\n",
            "transactions_rename.rename(columns={'customer_level_2':'customer_category', 'product_level_2':'product_category'}, inplace=True)\n",
            "transactions_rename.dtypes"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 4.6. Add columns"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 61,
         "metadata": {},
         "outputs": [],
         "source": [
            "customers_add_columns = customers.copy()\n",
            "\n",
            "# adding a new column to form a unique city name\n",
            "\n",
            "customers_add_columns[\"unique_city\"] = customers_add_columns[\"country\"] + \"-\" + customers_add_columns[\"city\"]\n",
            "\n",
            "customers_add_columns\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 4.7. Change data type"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 43,
         "metadata": {},
         "outputs": [],
         "source": [
            "transactions_type = transactions.copy()\n",
            "\n",
            "#To text\n",
            "transactions_type['row_id'] = transactions_type['row_id'].astype(str)\n",
            "transactions_type.dtypes\n",
            "\n",
            "#To number\n",
            "transactions_type['row_id'] = pd.to_numeric(transactions_type['row_id'], errors='coerce')\n",
            "transactions_type.dtypes\n",
            "\n",
            "#To date\n",
            "transactions_type['transaction_date'] = pd.to_datetime(transactions_type['transaction_date'])\n",
            "transactions_type.dtypes"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 5. Transpose (transpose)\n",
            "https://pandas.pydata.org/docs/user_guide/reshaping.html#reshaping-by-melt"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Pivots the orientation of the data so that horizontal fields are moved on the vertical axis"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 44,
         "metadata": {},
         "outputs": [],
         "source": [
            "# the costs dataset contains one column per month for product price in different geography:\n",
            "print(costs.info())\n",
            "costs.head(n=5)\n",
            "\n",
            "costs_transpose = pd.melt(costs, id_vars=['product_id','geo'], value_vars=['2019-01','2019-02','2019-03','2019-04','2019-05','2019-06','2019-07','2019-09','2019-10','2019-11','2019-12','2020-01'])\n",
            "costs_transpose.rename(columns={'variable':'year_month'}, inplace=True)\n",
            "costs_transpose.rename(columns={'value':'average_unit_cost'}, inplace=True)\n",
            "\n",
            "print(costs_transpose.info())\n",
            "costs_transpose.head(n=5)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 45,
         "metadata": {},
         "outputs": [],
         "source": [
            "#\n",
            "# if you do not want to type the list of columns to pivot, you can build it from the definition of the dataframe:\n",
            "#\n",
            "\n",
            "# get the list of columns as a Python list\n",
            "costs_columns_to_pivot = list(costs.columns)\n",
            "\n",
            "# remove the columns we want to retain as is\n",
            "costs_columns_to_pivot.remove(\"product_id\")\n",
            "costs_columns_to_pivot.remove(\"geo\")\n",
            "print(costs_columns_to_pivot)\n",
            "\n",
            "# transpose\n",
            "costs_transpose_2 = pd.melt(costs, id_vars=['product_id','geo'], value_vars=costs_columns_to_pivot)\n",
            "costs_transpose_2.rename(columns={'variable':'year_month'}, inplace=True)\n",
            "costs_transpose_2.rename(columns={'value':'average_unit_cost'}, inplace=True)\n",
            "\n",
            "# check results\n",
            "print(costs_transpose_2.info())\n",
            "costs_transpose_2.head(n=5)\n",
            "\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### 6. Merge datasets (join)\n",
            "https://pandas.pydata.org/docs/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 6.1. Examples of joins supported by the pandas merge() function"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The examples use two small dataframes (left_source_df and right_source_df) The join key is stored in a column named 'column_join' in both datasets, but this is not required. The merge() function will update the name of columns present in both datasets that are not part of the join key. For left join, right join and outer join, the merge() function sets missing values to NaN."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 47,
         "metadata": {},
         "outputs": [],
         "source": [
            "left_source_df = pd.DataFrame({\"column_join\": [\"A\",\"B\",\"C\"], \"column_b\": [\"X1\",\"Y1\",\"W1\"], \"column_c\":[111,222,333]})\n",
            "right_source_df = pd.DataFrame({\"column_join\": [\"A\",\"B\",\"D\"], \"column_b\": [\"X2\",\"Y2\",\"Z2\"], \"column_d\":[444,555,666]})\n",
            "\n",
            "print(\"\\nLeft source\")\n",
            "print(left_source_df)\n",
            "\n",
            "print(\"\\nRight source\")\n",
            "print(right_source_df)\n",
            "\n",
            "print(\"\\nExample inner join\")\n",
            "example_inner = pd.merge(left_source_df, right_source_df, \n",
            "                         how='inner', \n",
            "                         left_on=['column_join'], \n",
            "                         right_on=['column_join'], \n",
            "                         suffixes=('_LEFT', '_RIGHT'))\n",
            "print(example_inner)\n",
            "\n",
            "print(\"\\nExample left join\")\n",
            "example_left = pd.merge(left_source_df, right_source_df, \n",
            "                        how='left', left_on=['column_join'], \n",
            "                        right_on=['column_join'], \n",
            "                        suffixes=('_LEFT', '_RIGHT'))\n",
            "print(example_left)\n",
            "\n",
            "print(\"\\nExample right join\")\n",
            "example_right = pd.merge(left_source_df, right_source_df, \n",
            "                         how='right', \n",
            "                         left_on=['column_join'], \n",
            "                         right_on=['column_join'], \n",
            "                         suffixes=('_LEFT', '_RIGHT'))\n",
            "print(example_right)\n",
            "\n",
            "print(\"\\nExample outer join\")\n",
            "example_outer = pd.merge(left_source_df, right_source_df, \n",
            "                         how='outer', \n",
            "                         left_on=['column_join'], \n",
            "                         right_on=['column_join'], \n",
            "                         suffixes=('_LEFT', '_RIGHT'))\n",
            "print(example_outer)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 6.2. Using information returned by merge() to run right_only and left_only joins"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 48,
         "metadata": {},
         "outputs": [],
         "source": [
            "# the merge() function can add a column with the source of the data (left_only, right_only, both) \n",
            "# by default, this column is named \"_merge\"\n",
            "# we can apply a filter on this column to retain only rows available in left data source, right data source or both.\n",
            "\n",
            "print(\"\\nExample outer join with indicator\")\n",
            "example_outer_with_indicator = pd.merge(left_source_df, right_source_df, indicator=True, how='outer', left_on=['column_join'], right_on=['column_join'], suffixes=('_LEFT', '_RIGHT'))\n",
            "print(example_outer_with_indicator)\n",
            "\n",
            "print(\"\\nExample left only join\")\n",
            "example_left_only = pd.merge(left_source_df, right_source_df, indicator=True, how='left', left_on=['column_join'], right_on=['column_join'], suffixes=('_LEFT', '_RIGHT'))\n",
            "example_left_only = example_left_only[example_left_only[\"_merge\"] == \"left_only\"]\n",
            "print(example_left_only)\n",
            "\n",
            "print(\"\\nExample right only join\")\n",
            "example_right_only = pd.merge(left_source_df, right_source_df, indicator=True, how='right', left_on=['column_join'], right_on=['column_join'], suffixes=('_LEFT', '_RIGHT'))\n",
            "example_right_only = example_right_only[example_right_only[\"_merge\"] == \"right_only\"]\n",
            "print(example_right_only)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 49,
         "metadata": {},
         "outputs": [],
         "source": [
            "#\n",
            "# we can, of course, extract the three subsets from the results of our outer join with indicator\n",
            "#\n",
            "example_outer_left_only = example_outer_with_indicator[example_outer_with_indicator[\"_merge\"] == \"left_only\"]\n",
            "example_outer_right_only = example_outer_with_indicator[example_outer_with_indicator[\"_merge\"] == \"right_only\"]\n",
            "example_outer_common_only = example_outer_with_indicator[example_outer_with_indicator[\"_merge\"] == \"both\"]\n",
            "\n",
            "print(\"\\nExample right only from outer join\")\n",
            "print(example_outer_left_only)\n",
            "\n",
            "print(\"\\nExample right only from outer join\")\n",
            "print(example_outer_right_only)\n",
            "\n",
            "print(\"\\nExample common only from outer join\")\n",
            "print(example_outer_common_only)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 6.3. Data preparation for examples based on our transaction data"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "<div class=\"alert alert-block alert-danger\">\n",
            "For the merge code snippets to work, make sure to run first the transpose code snippet to generate the \"costs_transpose\" dataset, and also run the snippet here under to generate the \"transactions_enriched\" dataset \n",
            "</div>"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 50,
         "metadata": {},
         "outputs": [],
         "source": [
            "transactions_enriched = transactions.copy()\n",
            "transactions_enriched['transaction_date'] = transactions_enriched['transaction_date'].astype(str)\n",
            "transactions_enriched['year_month'] = transactions_enriched['transaction_date'].str[:7]\n",
            "transactions_enriched[['transaction_date', 'year_month']]\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 6.3. Running an inner join on our transaction data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 51,
         "metadata": {},
         "outputs": [],
         "source": [
            "merge_inner = pd.merge(transactions_enriched, costs_transpose, how='inner', left_on=['product_id', 'geo', 'year_month'], right_on=['product_id', 'geo', 'year_month'], suffixes=('_LEFT', '_RIGHT'))\n",
            "merge_inner.info()\n",
            "\n",
            "print(f\"Nunmber of rows in transaction data {transactions_enriched.shape[0]}\")\n",
            "print(f\"Number of rows in merged dataset: {merge_inner.shape[0]}\")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "We are loosing transaction records because the merge() function cannot find corresponding cost information. What we need here is a left join, something similat to a VLookup() in Excel"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 6.4. Running a left join ( VLookup() ) on our transaction data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 53,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Since the column names are identical in both dataframes, we can used a simplified version:\n",
            "\n",
            "merge_left = pd.merge(transactions_enriched, costs_transpose,\n",
            "                      indicator=True,\n",
            "                      how='left', \n",
            "                      on=['product_id', 'geo', 'year_month'], \n",
            "                      suffixes=('_LEFT', '_RIGHT'))\n",
            "\n",
            "merge_left.info()\n",
            "\n",
            "print(f\"Nunmber of rows in transaction data {transactions_enriched.shape[0]}\")\n",
            "print(f\"Number of rows in merged dataset: {merge_left.shape[0]}\")\n",
            "\n",
            "transactions_without_costs = merge_left[merge_left[\"_merge\"] == \"left_only\"]\n",
            "print(f\"Number of transaction rows without costs: {transactions_without_costs.shape[0]}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### 6.4. Running a right-only join to find product without any transactions"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 54,
         "metadata": {},
         "outputs": [],
         "source": [
            "# All columns from the transaction dataset will be set to NaN, so we retain only the columns used as key\n",
            "\n",
            " \n",
            "merge_right_only = pd.merge(transactions_enriched[['product_id', 'geo', 'year_month']], costs_transpose, \n",
            "                            indicator=True, \n",
            "                            how='right', \n",
            "                            on=['product_id', 'geo', 'year_month'], \n",
            "                            suffixes=('_LEFT', '_RIGHT'))\n",
            "\n",
            "merge_right_only = merge_right_only[merge_right_only[\"_merge\"] == \"right_only\"]\n",
            "\n",
            "merge_right_only\n",
            " "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### xx. Row by row transformations"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "In some cases, we need to run some code for each row. Here is a simple example."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 55,
         "metadata": {},
         "outputs": [],
         "source": [
            "customers_copy = customers.copy()\n",
            "#\n",
            "# for this example, we add the country code as a prefix to the phone number if the prefix is not yet there\n",
            "# there are a lot of things that can go wrong and are not covered hereunder.\n",
            "#\n",
            "\n",
            "for row_number, row_data in customers_copy.iterrows():\n",
            "   \n",
            "    phone_nbr = row_data[\"phone_nbr\"]\n",
            "    country = row_data[\"country\"]\n",
            "    \n",
            "    \n",
            "    if country == \"BE\":\n",
            "        prefix = \"0032\"\n",
            "    elif country == \"FR\":\n",
            "        prefix = \"0033\"\n",
            "    else:\n",
            "        prefix = \"\"\n",
            "        \n",
            "# update the dataframe, as required\n",
            "\n",
            "    if len(prefix) > 0 and phone_nbr[0:4] != prefix:\n",
            "        new_phone_nbr = prefix + phone_nbr\n",
            "        customers_copy.loc[row_number,\"phone_nbr\"] = new_phone_nbr\n",
            "\n",
            "print(customers_copy)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Write data (outputdata)\n",
            "https://pandas.pydata.org/docs/user_guide/io.html"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Write to csv"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "transactions.to_csv('data_output/transactions_output.csv', sep=',', header=True, index=False, encoding='utf-8')"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Write to xlsx"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "merge_inner.to_excel('data_output/merge.xlsx', sheet_name='merge_inner', index=False)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.8.8"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 1
}